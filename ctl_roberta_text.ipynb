{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF1sxaSKmT9A"
      },
      "source": [
        "Author: Daniel Low\n",
        "\n",
        "Based on tutorials:\n",
        "- https://github.com/huggingface/notebooks/blob/e1983033bf88432e1e371996e1deec2f6ef1c52a/examples/text_classification.ipynb\n",
        "- https://huggingface.co/blog/ray-tune\n",
        "- https://huggingface.co/docs/transformers/training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Dc8SGM0hkC",
        "outputId": "f55e27ab-7f8c-473d-c09f-6fe7aa06321f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "# python and package Versions used\n",
        "\n",
        "!python --version # I ran on python==3.10.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8PFR1gT-0d7t"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch==2.0.1 datasets==2.14.3 transformers==4.28.1 accelerate==0.15.0 optuna==3.2.0 evaluate\n",
        "#imbalanced-learn==0.11.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNzeFzgY0d7u",
        "outputId": "6a4b9d88-88b4-42a0-bde9-30d1855bf964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Please check your installation and if your hardware supports CUDA.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    # Print number of GPUs available\n",
        "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
        "\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}:\")\n",
        "        print(f\"\\tName: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"\\tCompute Capability: {torch.cuda.get_device_capability(i)}\")\n",
        "        print(f\"\\tTotal Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n",
        "        print(torch.cuda.get_device_properties(i))\n",
        "        # Additional details can be accessed via `torch.cuda.get_device_properties(i)`\n",
        "else:\n",
        "    print(\"CUDA is not available. Please check your installation and if your hardware supports CUDA.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_08Q5Rf1Wcqt",
        "outputId": "f68f64d0-b8a4-434d-bc5d-a5e0a286ae42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Authors: Daniel M. Low\n",
        "License: See license in github repository\n",
        "'''\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# pd.options.display.width = 0\n",
        "\n",
        "\n",
        "on_colab = True\n",
        "\n",
        "\n",
        "\n",
        "if on_colab:\n",
        "  from google.colab import drive\n",
        "  project_name = 'concept_tracker'\n",
        "  drive.mount('/content/drive')\n",
        "  input_dir = f'/content/drive/MyDrive/datum/{project_name}/data/ctl/'\n",
        "  output_dir = f'/content/drive/MyDrive/datum/{project_name}/data/output/lexicon_paper/'\n",
        "else:\n",
        "  input_dir = './data/ctl/'\n",
        "  output_dir = './data/output/lexicon_paper/'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wOxL5QWm0d7u"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from datasets.dataset_dict import DatasetDict\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset, load_metric\n",
        "from datasets import list_metrics\n",
        "from imblearn.over_sampling import RandomOverSampler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a text classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "toy = False\n",
        "regression = True # False for classification\n",
        "task = \"suicide_risk\"\n",
        "text_col = 'text'\n",
        "y_col = 'y'\n",
        "model_checkpoint = \"roberta-base\" # \"distilbert-base-uncased\"\n",
        "num_labels = 1 # Use 1 for regression\n",
        "metrics_to_report = ['Model',  'RMSE','RMSE per value','MAE','MAE per value', 'rho', 'gridsearch', 'Best parameters']\n",
        "metric_name = \"rmse\"\n",
        "validation_key = 'validation'\n",
        "imbalanced = False # if True, will use custom function. False = leave imabalance or balance through oversampling/undersampling/etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlcYdA8-0d7v"
      },
      "source": [
        "### training or hyperparameter search arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NTiDFLpB0d7v"
      },
      "outputs": [],
      "source": [
        "\n",
        "# if finetuning without hypeparameter search\n",
        "batch_size = 16 #not higher due to GPU memory usage limit issues\n",
        "if toy:\n",
        "    epochs = 1\n",
        "else:\n",
        "    epochs = 3\n",
        "\n",
        "\n",
        "# hyperparameter search args\n",
        "do_hyperparameter_search = True\n",
        "hyperparameter_shards = 10 # 10 means you use 10th of the data from hyperparameter search\n",
        "hyperparameter_search_n_trials = 10\n",
        "hyperparameter_search_direction = 'minimize' # maximize for classification metric, minimize for loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7SufSyMT0xvF"
      },
      "outputs": [],
      "source": [
        "# if searching with hyperparameter search, options are default of optuna or specified here:\n",
        "\n",
        "\n",
        "# default optuna decided by transformers: https://github.com/huggingface/transformers/blob/dcbfd93d7aeb14f8ff08a48866d2a68950d4c69a/src/transformers/trainer_utils.py#L248\n",
        "\n",
        "# https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py\n",
        "\n",
        "def optuna_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
        "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1,4, step=1),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_validation_set = True\n",
        "\n",
        "train = pd.read_csv(input_dir+'train_10perc_text_balanced.csv', index_col=0)\n",
        "val = pd.read_csv(input_dir+'val_3perc_text_balanced.csv', index_col=0)\n",
        "test = pd.read_csv(input_dir+'test_3perc_text_balanced.csv', index_col=0)\n",
        "print(train.shape, val.shape, test.shape)\n",
        "\n",
        "if balanced_validation_set:\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
        "  print(train.shape, val.shape, test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuRSvvu0Ags-",
        "outputId": "23a6d003-b952-42c4-e44a-6cab9369e767"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4129, 3) (1078, 3) (1081, 3)\n",
            "(3303, 3) (826, 3) (1081, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWEsMYA2a3Cz",
        "outputId": "5c724d6e-f9ec-43eb-c6b9-303c5b85329e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3280, 3) (1071, 3)\n",
            "(3280,) (3280,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train = train.dropna()\n",
        "val = val.dropna()\n",
        "test = test.dropna()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(train.shape, test.shape)\n",
        "\n",
        "if toy:\n",
        "  train = train.sample(frac = 0.1)\n",
        "  val = val.sample(frac = 0.1)\n",
        "  test = test.sample(frac = 0.1)\n",
        "\n",
        "\n",
        "X_train = train[text_col].values\n",
        "y_train = train[y_col].values\n",
        "\n",
        "X_val = val[text_col].values\n",
        "y_val = val[y_col].values\n",
        "\n",
        "X_test = test[text_col].values\n",
        "y_test = test[y_col].values\n",
        "\n",
        "# Oversample to match method of model with metadata approach\n",
        "# ros = RandomOverSampler(random_state=0)\n",
        "# X_train, y_train = ros.fit_resample(X_train.reshape(-1, 1), y_train)\n",
        "# X_train = X_train.flatten()\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "\n",
        "d = {'train':Dataset.from_dict({'label':y_train,'text':X_train}),\n",
        "     'validation':Dataset.from_dict({'label':y_val,'text':X_val}),\n",
        "     'test':Dataset.from_dict({'label':y_test,'text':X_test})\n",
        "     }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset = DatasetDict(d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdJjoNGk10C_",
        "outputId": "7d7b6189-9b79-4b92-a456-09068aa95ef6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.0, 4.0, 2.0, 4.0, 4.0, 4.0, 3.0, 4.0, 3.0, 4.0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset['train']['label'][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt07O_LYlCJC",
        "outputId": "490dc483-0bdf-4f93-af5c-aa26716c3681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-f75b8a59469e>:4: FutureWarning: list_metrics is deprecated and will be removed in the next major version of datasets. Use 'evaluate.list_evaluation_modules' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metrics_list = list_metrics()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215\n",
            "['accuracy', 'bertscore', 'bleu', 'bleurt', 'brier_score', 'cer', 'character', 'charcut_mt', 'chrf', 'code_eval', 'comet', 'competition_math', 'confusion_matrix', 'coval', 'cuad', 'exact_match', 'f1', 'frugalscore', 'glue', 'google_bleu', 'indic_glue', 'mae', 'mahalanobis', 'mape', 'mase', 'matthews_correlation', 'mauve', 'mean_iou', 'meteor', 'mse', 'nist_mt', 'pearsonr', 'perplexity', 'poseval', 'precision', 'r_squared', 'recall', 'rl_reliability', 'roc_auc', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'smape', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'ter', 'trec_eval', 'wer', 'wiki_split', 'xnli', 'xtreme_s', 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics', 'BucketHeadP65/confusion_matrix', 'BucketHeadP65/roc_curve', 'DaliaCaRo/accents_unplugged_eval', 'DarrenChensformer/eval_keyphrase', 'DarrenChensformer/relation_extraction', 'DoctorSlimm/bangalore_score', 'DoctorSlimm/kaushiks_criteria', 'Drunper/metrica_tesi', 'Felipehonorato/eer', 'Fritz02/execution_accuracy', 'GMFTBY/dailydialog_evaluate', 'GMFTBY/dailydialogevaluate', 'He-Xingwei/sari_metric', 'Ikala-allen/relation_extraction', 'JP-SystemsX/nDCG', 'Josh98/nl2bash_m', 'KevinSpaghetti/accuracyk', 'LottieW/accents_unplugged_eval', 'LuckiestOne/valid_efficiency_score', 'Merle456/accents_unplugged_eval', 'Muennighoff/code_eval_octopack', 'NCSOFT/harim_plus', 'Natooz/ece', 'Ndyyyy/bertscore', 'NikitaMartynov/spell-check-metric', 'NimaBoscarino/weat', 'Ochiroo/rouge_mn', 'Pipatpong/perplexity', 'Qui-nn/accents_unplugged_eval', 'RiciHuggingFace/accents_unplugged_eval', 'Soroor/cer', 'SpfIo/wer_checker', 'Splend1dchan/cosine_similarity', 'TelEl/accents_unplugged_eval', 'Vallp/ter', 'Vertaix/vendiscore', 'Vickyage/accents_unplugged_eval', 'Viona/fuzzy_reordering', 'Viona/infolm', 'Viona/kendall_tau', 'Vipitis/shadermatch', 'Vlasta/pr_auc', 'Yeshwant123/mcc', 'abdusah/aradiawer', 'abidlabs/mean_iou', 'abidlabs/mean_iou2', 'agkphysics/ccc', 'akki2825/accents_unplugged_eval', 'alvinasvk/accents_unplugged_eval', 'amitness/perplexity', 'andstor/code_perplexity', 'angelasophie/accents_unplugged_eval', 'angelina-wang/directional_bias_amplification', 'anz2/iliauniiccocrevaluation', 'arthurvqin/pr_auc', 'aryopg/roc_auc_skip_uniform_labels', 'bdsaglam/jer', 'boschar/accents_unplugged_eval', 'brian920128/doc_retrieve_metrics', 'bstrai/classification_report', 'bugbounty1806/accuracy', 'cakiki/ndcg', 'carletoncognitivescience/peak_signal_to_noise_ratio', 'chanelcolgate/average_precision', 'chimene/accents_unplugged_eval', 'ckb/unigram', 'codeparrot/apps_metric', 'cpllab/syntaxgym', 'd-matrix/dmx_perplexity', 'daiyizheng/valid', 'danieldux/hierarchical_softmax_loss', 'dayil100/accents_unplugged_eval', 'dayil100/accents_unplugged_eval_WER', 'dgfh76564/accents_unplugged_eval', 'dvitel/codebleu', 'ecody726/bertscore', 'erntkn/dice_coefficient', 'fnvls/bleu1234', 'fnvls/bleu_1234', 'fschlatt/ner_eval', 'gabeorlanski/bc_eval', 'giulio98/code_eval_outputs', 'giulio98/codebleu', 'gjacob/bertimbauscore', 'gjacob/chrf', 'gjacob/google_bleu', 'gjacob/wiki_split', 'gnail/cosine_similarity', 'gorkaartola/metric_for_tp_fp_samples', 'guydav/restrictedpython_code_eval', 'hack/test_metric', 'harshhpareek/bertscore', 'hpi-dhc/FairEval', 'huanghuayu/multiclass_brier_score', 'hynky/sklearn_proxy', 'hyperml/balanced_accuracy', 'idsedykh/codebleu', 'idsedykh/codebleu2', 'idsedykh/megaglue', 'idsedykh/metric', 'illorca/FairEval', 'ingyu/klue_mrc', 'jialinsong/apps_metric', 'jjkim0807/code_eval', 'jordyvl/ece', 'jpxkqx/peak_signal_to_noise_ratio', 'jpxkqx/signal_to_reconstruction_error', 'juliakaczor/accents_unplugged_eval', 'jzm-mailchimp/joshs_second_test_metric', 'k4black/codebleu', 'kashif/mape', 'kedudzic/charmatch', 'kyokote/my_metric2', 'langdonholmes/cohen_weighted_kappa', 'leslyarun/fbeta_score', 'lhy/hamming_loss', 'lhy/ranking_loss', 'livvie/accents_unplugged_eval', 'loubnabnl/apps_metric2', 'lvwerra/accuracy_score', 'lvwerra/bary_score', 'lvwerra/test', 'manueldeprada/beer', 'mfumanelli/geometric_mean', 'mgfrantz/roc_auc_macro', 'mtc/fragments', 'nevikw39/specificity', 'nlpln/tst', 'ola13/precision_at_k', 'omidf/squad_precision_recall', 'posicube/mean_reciprocal_rank', 'repllabs/mean_average_precision', 'repllabs/mean_reciprocal_rank', 'ronaldahmed/nwentfaithfulness', 'sakusakumura/bertscore', 'shalakasatheesh/squad', 'shalakasatheesh/squad_v2', 'shirayukikun/sescore', 'shunzh/apps_metric', 'sma2023/wil', 'sportlosos/sescore', 'transZ/sbert_cosine', 'transZ/test_parascore', 'transformersegmentation/segmentation_scores', 'unitxt/metric', 'unnati/kendall_tau_distance', 'vichyt/metric-codebleu', 'weiqis/pajm', 'xu1998hz/sescore', 'xu1998hz/sescore_english_coco', 'xu1998hz/sescore_english_mt', 'xu1998hz/sescore_english_webnlg', 'xu1998hz/sescore_german_mt', 'ybelkada/cocoevaluate', 'yonting/average_precision_score', 'yqsong/execution_accuracy', 'yulong-me/yl_metric', 'yuyijiong/quad_match_score', 'yzha/ctc_eval', 'zbeloki/m2']\n"
          ]
        }
      ],
      "source": [
        "from evaluate import load\n",
        "\n",
        "\n",
        "metrics_list = list_metrics()\n",
        "print(len(metrics_list))\n",
        "print(metrics_list)\n",
        "if regression:\n",
        "  metric = load('mse')\n",
        "else:\n",
        "  metric = load_metric(\"f1\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SZy5tRB_IrI7"
      },
      "outputs": [],
      "source": [
        "# show_random_elements(dataset[\"train\"], num_examples=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "You can call its `compute` method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XN1Rq0aIrJC",
        "outputId": "42acf8ef-9803-4f48-efbe-9fedcf37ea98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mse': 0.40625}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "fake_preds = np.random.randint(0, 2, size=(64,))\n",
        "fake_labels = np.random.randint(0, 2, size=(64,))\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXNLu_-nIrJI",
        "outputId": "4a6b29d4-5cc9-40c0-88fe-24d61bc95e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5hBlsrHIrJL",
        "outputId": "5a852616-d839-4815-cbad-36e3c88f2c44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [0, 31414, 6, 42, 65, 3645, 328, 2, 2, 2409, 42, 3645, 1411, 19, 24, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_0B1M2IrJM"
      },
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fyGdtK9oIrJM"
      },
      "outputs": [],
      "source": [
        "\n",
        "task_to_keys = {\n",
        "    'stb': (\"text\", None),\n",
        "    'suicide_risk': (\"text\", None),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbqtC4MrIrJO"
      },
      "source": [
        "We can double check it does work on our current dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19GG646uIrJO",
        "outputId": "7d770b30-4e00-4c16-9f8b-b9f07504f1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Hey, I'm about to off myself and want somebody to stop me. Like I said, I'm feeling terrible. I'm Worth. Yeah, razors. Yeah, in my pocket. Okay, that's okay. I put them in the couch. Right now. My life was perfect, and I fucked it up by being a drunk piece of shit. I was gonna get the medicine I need, hormones. But now I've risked all that. Okay, I go over to my friends house. We get drunk. I wake up, my mom nursing me in bed. I'm afraid what my parents will do to me. I want to see the doctor, if I can't get that, I'll kill myself. My parents, I'm afraid it's how they'll punish me. Some good, some bad. If my parents [scrubbed] me this medicine, I'll kill myself to get back at them. Yeah, a lot, especially when trying to get this medicine. I tried and failed to hang myself once. Stupid stupid queer. Thank you, that means a lot. Better, but still not good. My mom said she probably won't take my medicine away. If she does take it away, I will kill myself. Yeah, we've talked a bit about what a lying piece of shit I am. She brushed over the idea of not taking my medicine away. Thank you, that means a lot. I want to self harm a lot so I don't have to deal with my parents. I use alcohol or play video games. But I want to quit drinking now. Yes, I will. But now I have a bad voice in my head telling me to hurt my parents. Ignore him. He's a mean person. He has me leave class and hurt myself. He's mean, and I don't care what he thinks, he just screams at me so loud. I focus on something else, even when he starts shouting. I had to to talk about it with my doctor. That would be nice, thank you. No, not at all, I'd probably get hurt. My Dad screamed at me and took my books away because I hesrd the voices. And the voices tell me bad things. To hurt them. Not that I remember. Yes, a psychologist and psychiatrist. Yes. I do find them helpful. I'm not sure. Yes, but I can control it. Distractions and shouting back at the voices. I'm not sure if I can. I like talking to you people on the crisis line. I can keep that. I'm not feeling very good tonight. I need a real person to listen to me who isn't my mom or dad. Nothing, my parents have me backed into a corner. All I can do is shout down the bad voices. The bad voice, [scrubbed], especially. My parents won't let me have some time alone to process stuff. I listen to music when I can and watch tv when I can. That's totally right. I appreciate that so much. When I talk to real people, bad voices can't get to me. The medicine I need. I hope my mom lets me have it tonight. Yes, if I [scrubbed] her she might get angry. the mean woman will come take all my medicine away. Yes, I would. I'd be very compelled to harm myself in that case. But now I use this number. Thank you. And you never get angry at me or call me a liar like mom or dad. And you never attack me or shout at the dog or make me feel bad. Thank you. I still want to talk. I spend all my time doing nothing. And my Dad yells at me. He curses at me. I'll eat some food, sleep. Yes, that's a good plan, thank you for talking to me. Do you want me to end the conversation or can you do it?\n"
          ]
        }
      ],
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "\n",
        "\n",
        "\n",
        "if sentence2_key is None:\n",
        "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
        "else:\n",
        "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
        "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    if sentence2_key is None:\n",
        "        return tokenizer(examples[sentence1_key], truncation=True)\n",
        "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b70jh26IrJS",
        "outputId": "408a7d63-9132-4b2f-ab33-bfca66e7e5ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 13368, 6, 38, 437, 59, 7, 160, 2185, 8, 236, 4909, 7, 912, 162, 4, 2011, 38, 26, 6, 38, 437, 2157, 6587, 4, 38, 437, 13399, 4, 8976, 6, 910, 1222, 994, 4, 8976, 6, 11, 127, 7524, 4, 8487, 6, 14, 18, 8578, 4, 38, 342, 106, 11, 5, 16433, 4, 5143, 122, 4, 1308, 301, 21, 1969, 6, 8, 38, 42647, 24, 62, 30, 145, 10, 10789, 2125, 9, 15328, 4, 38, 21, 6908, 120, 5, 6150, 38, 240, 6, 25641, 4, 125, 122, 38, 348, 25892, 70, 14, 4, 8487, 6, 38, 213, 81, 7, 127, 964, 790, 4, 166, 120, 10789, 4, 38, 3874, 62, 6, 127, 3795, 8701, 162, 11, 3267, 4, 38, 437, 6023, 99, 127, 1041, 40, 109, 7, 162, 4, 38, 236, 7, 192, 5, 3299, 6, 114, 38, 64, 75, 120, 14, 6, 38, 581, 3549, 2185, 4, 1308, 1041, 6, 38, 437, 6023, 24, 18, 141, 51, 581, 15392, 162, 4, 993, 205, 6, 103, 1099, 4, 318, 127, 1041, 646, 3866, 31413, 5134, 742, 162, 42, 6150, 6, 38, 581, 3549, 2185, 7, 120, 124, 23, 106, 4, 8976, 6, 10, 319, 6, 941, 77, 667, 7, 120, 42, 6150, 4, 38, 1381, 8, 1447, 7, 6713, 2185, 683, 4, 44462, 12103, 22240, 4, 3837, 47, 6, 14, 839, 10, 319, 4, 11238, 6, 53, 202, 45, 205, 4, 1308, 3795, 26, 79, 1153, 351, 75, 185, 127, 6150, 409, 4, 318, 79, 473, 185, 24, 409, 6, 38, 40, 3549, 2185, 4, 8976, 6, 52, 348, 3244, 10, 828, 59, 99, 10, 6480, 2125, 9, 15328, 38, 524, 4, 264, 23637, 81, 5, 1114, 9, 45, 602, 127, 6150, 409, 4, 3837, 47, 6, 14, 839, 10, 319, 4, 38, 236, 7, 1403, 4798, 10, 319, 98, 38, 218, 75, 33, 7, 432, 19, 127, 1041, 4, 38, 304, 3766, 50, 310, 569, 426, 4, 125, 38, 236, 7, 6602, 4835, 122, 4, 3216, 6, 38, 40, 4, 125, 122, 38, 33, 10, 1099, 2236, 11, 127, 471, 2758, 162, 7, 2581, 127, 1041, 4, 45090, 123, 4, 91, 18, 10, 1266, 621, 4, 91, 34, 162, 989, 1380, 8, 2581, 2185, 4, 91, 18, 1266, 6, 8, 38, 218, 75, 575, 99, 37, 4265, 6, 37, 95, 26776, 23, 162, 98, 7337, 4, 38, 1056, 15, 402, 1493, 6, 190, 77, 37, 2012, 14487, 4, 38, 56, 7, 7, 1067, 59, 24, 19, 127, 3299, 4, 280, 74, 28, 2579, 6, 3392, 47, 4, 440, 6, 45, 23, 70, 6, 38, 1017, 1153, 120, 2581, 4, 1308, 13404, 24509, 23, 162, 8, 362, 127, 2799, 409, 142, 38, 36279, 2586, 5, 6820, 4, 178, 5, 6820, 1137, 162, 1099, 383, 4, 598, 2581, 106, 4, 1491, 14, 38, 2145, 4, 3216, 6, 10, 19902, 8, 27321, 4, 3216, 4, 38, 109, 465, 106, 7163, 4, 38, 437, 45, 686, 4, 3216, 6, 53, 38, 64, 797, 24, 4, 11281, 29866, 8, 14487, 124, 23, 5, 6820, 4, 38, 437, 45, 686, 114, 38, 64, 4, 38, 2], [0, 31414, 4, 272, 19864, 6225, 4260, 4, 2647, 38, 236, 7, 1597, 142, 38, 64, 75, 1149, 751, 396, 82, 584, 383, 59, 162, 8, 38, 437, 95, 7428, 4, 8976, 4, 38, 33, 13866, 8, 3766, 4, 272, 19864, 213, 7, 3581, 4, 3216, 4, 5148, 4, 2477, 142, 38, 33, 7, 213, 124, 7, 173, 3859, 127, 1131, 989, 3587, 4, 38, 4443, 98, 4, 440, 65, 1516, 24, 4, 3216, 38, 524, 4, 38, 218, 75, 236, 7, 989, 5, 790, 142, 82, 224, 383, 59, 127, 2772, 5861, 4, 38, 2220, 75, 57, 7, 5, 558, 187, 494, 8, 38, 269, 524, 45, 546, 556, 7, 164, 124, 4, 38, 218, 75, 206, 38, 64, 3679, 24, 4, 252, 214, 15983, 103, 32, 19887, 8, 13736, 219, 4, 440, 89, 965, 75, 4, 1308, 1441, 4, 125, 70, 38, 437, 174, 16, 24, 18, 11, 110, 471, 4, 6834, 16, 741, 29, 4, 407, 38, 218, 75, 224, 932, 143, 1181, 4, 252, 64, 75, 4, 38, 33, 10, 18931, 4, 8976, 686, 4, 1491, 682, 4, 38, 218, 75, 2145, 4, 38, 437, 259, 4, 38, 218, 75, 216, 4, 2], [0, 19457, 4, 23384, 8, 6882, 4, 646, 3866, 31413, 5134, 742, 38, 33, 239, 13838, 6943, 4, 407, 38, 218, 75, 2662, 11, 3267, 70, 183, 8, 486, 160, 31, 173, 6, 1386, 38, 173, 2185, 910, 11290, 6, 53, 13582, 38, 437, 10, 7319, 4, 38, 64, 75, 1137, 1268, 38, 437, 10, 7319, 6, 269, 4, 2011, 6, 38, 64, 75, 486, 160, 173, 50, 185, 6383, 4, 38, 15407, 13, 19858, 142, 38, 619, 101, 38, 33, 117, 2031, 4, 125, 24, 67, 4685, 39296, 3722, 32167, 4, 2306, 50, 223, 11012, 4, 38, 56, 65, 454, 79, 1410, 7, 277, 558, 8, 38, 1299, 38, 21, 608, 157, 615, 8, 399, 75, 619, 62, 7, 1158, 70, 81, 456, 4, 38, 129, 682, 342, 24, 561, 4, 19576, 38, 218, 75, 216, 141, 7, 4190, 24, 6, 8, 38, 269, 218, 75, 619, 101, 38, 64, 386, 81, 4, 1308, 1784, 6, 53, 37, 129, 269, 25245, 172, 11620, 162, 7, 213, 124, 7, 5804, 4, 27672, 1496, 352, 4, 38, 657, 123, 8, 37, 6138, 162, 6, 53, 1135, 39, 3618, 11, 21968, 21370, 37, 630, 75, 120, 24, 4, 11733, 7, 10, 18931, 16, 3013, 26, 87, 626, 4, 407, 38, 437, 41, 25160, 8340, 8, 14, 2607, 6, 38, 67, 3116, 6, 2451, 6, 646, 3866, 31413, 5134, 7479, 8, 3836, 4, 125, 45, 2230, 77, 38, 437, 447, 2185, 5373, 4, 38, 437, 10, 9405, 7182, 2681, 16746, 6, 25, 157, 25, 4472, 7449, 4, 38, 216, 141, 7, 1920, 2185, 4, 598, 5, 3543, 53, 7154, 81, 24, 4, 407, 38, 554, 142, 9, 284, 9061, 6, 53, 5426, 77, 38, 21, 504, 14, 24, 1147, 127, 6943, 4, 38, 2294, 13, 10, 150, 8, 3491, 389, 23246, 8, 685, 10, 319, 9, 4, 12156, 966, 4, 407, 38, 554, 878, 456, 4, 38, 21, 5912, 8057, 149, 6585, 8, 190, 25, 41, 4194, 4, 38, 956, 402, 34812, 12685, 8, 1508, 10086, 4, 38, 348, 802, 59, 24, 4, 5359, 1056, 122, 15, 127, 7967, 8, 423, 38, 1437, 5, 15052, 9, 127, 7182, 22166, 20146, 683, 38, 348, 2325, 11, 10, 367, 55, 4, 38, 95, 218, 75, 236, 7, 13582, 81, 8, 223, 9046, 6000, 4, 178, 127, 1784, 40, 477, 24, 66, 6, 190, 600, 38, 216, 38, 348, 626, 24, 4, 38, 109, 24, 23, 173, 4, 38, 28930, 23, 127, 449, 6502, 139, 13, 410, 383, 172, 4883, 38, 33, 4, 38, 218, 75, 216, 141, 7, 4, 38, 460, 460, 912, 8, 1137, 127, 2173, 14, 38, 437, 45, 5800, 23, 123, 6, 38, 95, 300, 5800, 23, 5, 1068, 6, 53, 37, 18, 155, 4, 38, 437, 10, 205, 295, 13749, 6, 38, 95, 240, 7, 120, 81, 209, 4289, 743, 4, 1308, 1784, 300, 7758, 142, 38, 19475, 23, 10, 21342, 4, 2011, 6, 24, 18, 12103, 2682, 8, 4146, 9, 24, 16, 3031, 6378, 4, 38, 437, 45, 888, 5800, 19, 646, 3866, 31413, 5134, 7479, 38, 95, 9046, 350, 1335, 2], [0, 1185, 5, 23630, 516, 244, 2837, 28749, 1308, 1486, 16, 38, 218, 75, 23126, 697, 5988, 4, 23129, 8578, 116, 1599, 75, 486, 162, 2788, 162, 4, 2647, 24, 554, 94, 76, 77, 127, 4252, 962, 172, 38, 1410, 11, 19, 127, 30894, 8, 79, 630, 75, 575, 59, 162, 50, 127, 6453, 98, 38, 554, 3931, 38, 1381, 2758, 127, 3795, 59, 24, 53, 79, 8266, 24, 4, 38, 236, 7, 1597, 53, 23, 5, 276, 86, 38, 218, 75, 23126, 989, 127, 275, 964, 127, 4758, 8, 2335, 53, 38, 67, 218, 75, 23126, 697, 5988, 4, 1491, 269, 129, 65, 9, 106, 216, 59, 24, 4, 19079, 9, 498, 4, 20847, 10, 7023, 7, 127, 14599, 50, 4835, 127, 30894, 18, 3766, 19976, 38, 1597, 50, 4835, 14784, 16126, 4, 8976, 4, 20847, 10, 7023, 7, 127, 14599, 4, 520, 38, 1004, 379, 50, 504, 4, 3292, 8, 2600, 19, 127, 2335, 8, 4758, 4, 19719, 59, 38, 21, 562, 402, 7, 3529, 4, 2], [0, 19457, 4, 38, 619, 23630, 4, 38, 489, 2053, 59, 2398, 8, 489, 562, 10, 319, 9, 3156, 9, 162, 2429, 127, 1403, 11, 3617, 1319, 4, 3216, 38, 619, 101, 608, 24, 4, 38, 95, 269, 218, 75, 236, 7, 2581, 127, 2638, 1980, 50, 3211, 409, 127, 499, 98, 38, 437, 2818, 38, 64, 489, 2185, 31, 41, 15095, 5593, 3320, 4, 38, 533, 64, 38, 437, 95, 45, 1402, 8, 619, 101, 38, 64, 75, 4, 29251, 1980, 33, 16270, 11, 4, 2011, 19, 10, 794, 19, 21550, 19, 13866, 9755, 160, 10, 4081, 4, 208, 5895, 154, 127, 36093, 27916, 6721, 10, 646, 3866, 31413, 5134, 742, 88, 127, 19444, 149, 127, 2295, 8, 4285, 41112, 643, 11, 3553, 364, 94, 1946, 50, 540, 4, 345, 16, 10, 794, 11, 5, 929, 159, 5, 646, 3866, 31413, 5134, 742, 36063, 11, 127, 929, 67, 794, 2185, 608, 14, 4, 178, 21550, 28239, 15, 5, 4647, 4, 40649, 9, 4, 16210, 498, 24, 16270, 88, 127, 1508, 14, 38, 1017, 3549, 2185, 3859, 172, 38, 802, 4091, 9179, 24, 4, 125, 38, 1682, 2758, 2185, 117, 8, 45, 7, 4, 38, 56, 14, 1369, 2350, 36, 17643, 43, 8, 3553, 16, 662, 4, 38, 190, 553, 2185, 473, 14, 1266, 452, 50, 302, 116, 345, 16, 10, 411, 864, 4260, 36914, 5, 55, 4420, 5274, 5, 2789, 7, 4260, 4, 38, 348, 57, 4889, 23, 80, 50, 130, 13, 10, 76, 8, 10, 457, 4, 38, 67, 524, 747, 15, 3553, 364, 17563, 9, 204, 8, 33, 1348, 195, 137, 4, 38, 216, 70, 42, 142, 38, 33, 10, 403, 5015, 54, 6990, 162, 167, 1142, 4, 16923, 24, 18, 57, 95, 81, 10, 76, 8, 10, 457, 53, 202, 4, 20, 754, 38, 218, 75, 236, 7, 2581, 127, 2638, 1980, 8, 5, 754, 38, 236, 7, 10732, 5, 2799, 38, 348, 57, 2410, 98, 127, 3768, 218, 75, 1597, 19, 162, 4, 4642, 127, 3795, 64, 75, 4960, 10, 6172, 48088, 5, 1079, 9, 127, 284, 16, 982, 409, 98, 19313, 74, 28, 182, 543, 4, 1578, 127, 820, 1187, 4115, 16, 15, 307, 98, 14, 74, 146, 24, 55, 8661, 13, 167, 54, 575, 59, 162, 4, 3216, 8, 117, 4, 85, 473, 8, 172, 24, 129, 29223, 1626, 162, 24, 1411, 258, 1319, 53, 6329, 4420, 4, 38, 218, 75, 216, 4, 10385, 16, 447, 4, 1944, 87, 5, 2788, 516, 53, 38, 202, 619, 101, 38, 236, 7, 1597, 4, 404, 38, 64, 206, 16, 7, 120, 5, 36063, 66, 9, 127, 929, 4, 6834, 38, 95, 222, 4, 38, 222, 4, 83, 367, 475, 28941, 124, 38, 156, 10, 889, 38, 5305, 9, 2188, 38, 236, 7, 1597, 8, 21, 2053, 59, 24, 3422, 4, 38, 206, 14, 8180, 155, 360, 137, 38, 1004, 820, 95, 1606, 18566, 7, 1356, 53, 38, 489, 2053, 59, 24, 4, 20294, 95, 2638, 1980, 8, 127, 2799, 14, 16, 59, 70, 89, 16, 235, 122, 4, 178, 1207, 259, 38, 437, 444, 31, 70, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "\n",
        "preprocess_function(dataset['train'][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "e2f08397badc4b27b1cffad695384468",
            "6578001f8c444cb4b6b5d367ab2a0c44",
            "d87614ecc68046feab06b15ead9f0ca9",
            "25072428bfb8439f8198adc7a0fd7abd",
            "805f44bd4f88406a96a3faed2ed0b891",
            "3cc44e5f1c6544f68d564317c42a74e2",
            "8ce0a8aa32c84c54950cb9e996b2d4c1",
            "a010f36dd5f74aea8faa28dc07d68f86",
            "a1606121df3d4b2ea4054b28fc88d1f8",
            "570c8bd4369240c2ab125c467b0580dc",
            "44b3b51af6d84c63af8dbadf54a2f97f",
            "83029eaf472d46818def81c2e8264df7",
            "117cdce9e8f54ab9b0b267ca2dd59da5",
            "9f611d0811e64f83b305d2702ed87d6b",
            "f60dbe5330044d65bdc145cf02074767",
            "693a66ac796b463380fdf9367dadda50",
            "af986b9f32724dad993c4c94280d1591",
            "993e6da535c64bf5b360e2e91c486ada",
            "a051496948be45b1ae5ff23f18a7343d",
            "2a2407b1a2294b0aa3aa315519349607",
            "392d84ccfce94e2d8092ef77c6c9b890",
            "d76ad8124dac408f80391205fd42cdda",
            "4fb39908b6c24f05ba94ffdb05b14576",
            "daf74c75e9ca476ba612ca51a1bed8a4",
            "00eca85de75143e5a9629938a1ad8443",
            "141893389be84c42aef3d5e8bfdf4dd8",
            "022d996cb35e462da753a5df523bb7a3",
            "46935bbc83d14cf78b29a34c117fb453",
            "cf1620ae14bb465cb74333354233881d",
            "f00b599a6ac3420cbc6a496e7172d45d",
            "76c332f0f3314aa2a91f57ff7b629378",
            "dc52b35d522f4a73b7bd8ecea2296765",
            "9a40a880c68e46c2836c548d41df13ea"
          ]
        },
        "id": "DDtsaJeVIrJT",
        "outputId": "b85d4855-5d9b-4a31-8c1d-f0f2163529be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3280 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2f08397badc4b27b1cffad695384468"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/823 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83029eaf472d46818def81c2e8264df7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1071 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fb39908b6c24f05ba94ffdb05b14576"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "encoded_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "Even better, the results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model with or without hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fcT7B52c0d7y"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, recall_score, precision_score, f1_score, auc, precision_recall_curve, classification_report\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     from evaluate import load\n",
        "#     predictions, labels = eval_pred\n",
        "#     metric = load('mse')\n",
        "#     return metric.compute(predictions=predictions, references=labels, squared=False)\n",
        "\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     if task != \"stsb\":\n",
        "#         predictions = np.argmax(predictions, axis=1)\n",
        "#     else:\n",
        "#         predictions = predictions[:, 0]\n",
        "#     return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "def metrics_report_cm(y_true, y_pred, output_dir, model_name, ts, save=True):\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred,normalize=None)\n",
        "    cm_df = pd.DataFrame(cm, index=['SITB-', 'SITB+'], columns=['SITB-', 'SITB+'])\n",
        "    cm_df_meaning = pd.DataFrame([['TN', 'FP'],['FN','TP']], index=['SITB-', 'SITB+'], columns=['SITB-', 'SITB+'])\n",
        "\n",
        "    cm_norm = confusion_matrix(y_true, y_pred,normalize='all')\n",
        "    cm_norm = (cm_norm*100).round(2)\n",
        "    cm_df_norm = pd.DataFrame(cm_norm, index=['SITB-', 'SITB+'], columns=['SITB-', 'SITB+'])\n",
        "\n",
        "\n",
        "    plt.rcParams['figure.figsize'] = [4,4]\n",
        "    cm_display = ConfusionMatrixDisplay(cm_norm,display_labels=['SITB-', 'SITB+']).plot()\n",
        "    # todo save\n",
        "\n",
        "    if save:\n",
        "        cm_df_meaning.to_csv(output_dir+f'cm_meaning_{model_name}_{ts}.csv')\n",
        "        cm_df.to_csv(output_dir+f'cm_{model_name}_{ts}.csv')\n",
        "        cm_df_norm.to_csv(output_dir+f'cm_norm_{model_name}_{ts}.csv')\n",
        "\n",
        "\n",
        "\n",
        "    return cm_df_meaning, cm_df, cm_df_norm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def metrics_report_classification_report(y_true, y_pred,y_pred_proba_1, output_dir, model_name, ts):\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    np.set_printoptions(suppress=True)\n",
        "    roc_auc = roc_auc_score(y_true,y_pred)\n",
        "    f1 = f1_score(y_true,y_pred)\n",
        "\n",
        "    # calculate precision and recall for each threshold\n",
        "    lr_precision, lr_recall, thresholds = precision_recall_curve(y_true, y_pred_proba_1)\n",
        "\n",
        "    # TODO: add best threshold\n",
        "    fscore = (2 * lr_precision * lr_recall) / (lr_precision + lr_recall)\n",
        "    fscore[np.isnan(fscore)] = 0\n",
        "    ix = np.argmax(fscore)\n",
        "    best_threshold = thresholds[ix].item()\n",
        "\n",
        "\n",
        "    pr_auc = auc(lr_recall, lr_precision)\n",
        "    # AU P-R curve is also approximated by avg. precision\n",
        "    # avg_pr = metrics.average_precision_score(y_true,y_pred_proba_1)\n",
        "\n",
        "    sensitivity = recall_score(y_true,y_pred)\n",
        "    specificity = tn / (tn+fp) # OR: recall_score(y_true,y_pred, pos_label=0)\n",
        "    precision = precision_score(y_true,y_pred)\n",
        "\n",
        "    results = pd.DataFrame([sensitivity, specificity,precision,f1, roc_auc,pr_auc, best_threshold],\n",
        "                        index = ['Sensitivity', 'Specificity', 'Precision', 'F1', 'ROC AUC','PR AUC', 'Best th PR AUC']).T.round(2)\n",
        "\n",
        "    results.to_csv(output_dir+f'results_{model_name}_{ts}.csv')\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import metrics\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "\n",
        "def regression_report(y_test,y_pred,y_train=None,gridsearch=None, best_params=None,feature_vector=None,model_name=None,metrics_to_report = 'all', plot = True, save_fig_path = None, round_to = 2):\n",
        "    '''\n",
        "    metrics = {'all', ['MAE','RMSE','rho', 'Best parameters']\n",
        "    }\n",
        "    '''\n",
        "\n",
        "    # Metrics\n",
        "    # https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
        "    rmse = metrics.mean_squared_error(y_test, y_pred, squared=False )\n",
        "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
        "    r2 = metrics.r2_score(y_test, y_pred)\n",
        "    r, p = pearsonr(y_test, y_pred)\n",
        "    rho, p = spearmanr(y_test, y_pred)\n",
        "\n",
        "\n",
        "    results_dict = {\n",
        "        'Model':f\"{feature_vector} {model_name}\",\n",
        "        'y_train_min': np.min(y_train),\n",
        "        'y_train_max': np.max(y_train),\n",
        "        'RMSE':np.round(rmse,round_to ),\n",
        "        'MAE':np.round(mae,round_to ),\n",
        "        'R^2':np.round(r2,round_to ),\n",
        "        'r':np.round(r,round_to ),\n",
        "        'rho':np.round(r,round_to ),\n",
        "        'gridsearch':gridsearch,\n",
        "        'Best parameters': str(best_params),\n",
        "        }\n",
        "    results = pd.DataFrame(results_dict, index=[model_name]).round(3)\n",
        "    # results_all.append(results)\n",
        "    if metrics != 'all':\n",
        "        if 'RMSE per value' in metrics_to_report or 'MAE per value' in metrics_to_report:\n",
        "            y_pred_test = {}\n",
        "            y_pred_test['RMSE per value'] = []\n",
        "            y_pred_test['MAE per value'] = []\n",
        "            for value in np.unique(y_test):\n",
        "                y_pred_test_i = [[pred,test] for pred,test in zip(y_pred,y_test) if test == value]\n",
        "                y_pred_test[value] = np.array(y_pred_test_i)\n",
        "                y_pred_i = [n[0] for n in y_pred_test_i]\n",
        "                y_test_i = [n[1] for n in y_pred_test_i]\n",
        "                rmse_i = metrics.mean_squared_error(y_test_i, y_pred_i, squared=False )\n",
        "                mae_i = metrics.mean_absolute_error(y_test_i, y_pred_i)\n",
        "                y_pred_test['RMSE per value'].append(np.round(rmse_i,round_to ))\n",
        "                y_pred_test['MAE per value'].append(np.round(mae_i,round_to ))\n",
        "            print(y_pred_test['RMSE per value'])\n",
        "            results_dict.update({\n",
        "            'RMSE per value':f\"{results_dict['RMSE']} {y_pred_test['RMSE per value']}\",\n",
        "            'MAE per value':f\"{results_dict['MAE']} {y_pred_test['MAE per value']}\"\n",
        "            })\n",
        "            metrics_to_report_2 = metrics_to_report.copy()\n",
        "            metrics_to_report_2.remove('RMSE') #redudant\n",
        "            metrics_to_report_2.remove('MAE') #redudant\n",
        "            results = pd.DataFrame(results_dict, index=[model_name]) # replace with updated metrics\n",
        "            results = results[metrics_to_report_2]\n",
        "        else:\n",
        "            results = pd.DataFrame(results_dict, index=[model_name]) # replace with updated metrics\n",
        "            results = results[metrics_to_report]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Plot result for a regression task: true value vs predicted values\n",
        "    # ============================================================\n",
        "    plt.clf()\n",
        "    plt.style.use('default')  # Example of applying the 'ggplot' style\n",
        "    plt.scatter(y_test, y_pred, alpha = 0.1)\n",
        "    plt.title(f\"RMSE: {results_dict['RMSE']}\\nMAE: {results_dict['MAE']}\\nrho: {results_dict['rho']}\"\n",
        "            )\n",
        "    plt.xlabel('True values')\n",
        "    plt.ylabel('Predicted values')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_fig_path:\n",
        "        plt.savefig(save_fig_path+'.png', dpi=150)\n",
        "    plt.show()\n",
        "    return results\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `AutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmP65aNS0d7y"
      },
      "source": [
        "### Custom Trainer to add class weights if you have class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFw1-qEB0d7y",
        "outputId": "5c6ad7f8-a6ca-4bc9-d906-d7f1aadd0d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.98944193 1.03339635 0.97881229]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from torch import nn\n",
        "from transformers import Trainer\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights =  compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_train), y= y_train)\n",
        "print(class_weights)\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # compute custom loss (suppose one has 3 labels with different weights)\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, device=model.device).float())\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNxrpAkW0d7y"
      },
      "source": [
        "### define or download model and define training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlqNaB8jIrJW",
        "outputId": "6194e4ed-c1fa-4375-dce1-47f1a95b9795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.11 ms, sys: 26 Âµs, total: 4.14 ms\n",
            "Wall time: 11.9 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Finetuning no hyperparameter search:\n",
        "# distilroberta 18m (4 epochs)\n",
        "# roberta-base 35m (3 epochs)\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "\n",
        "    predictions, labels = eval_pred\n",
        "    if regression:\n",
        "      rmse = mean_squared_error(labels, predictions, squared=False)\n",
        "      return {\"rmse\": rmse}\n",
        "    else:\n",
        "      if task != \"stb\":\n",
        "          predictions = np.argmax(predictions, axis=1)\n",
        "      else:\n",
        "          predictions = predictions[:, 0]\n",
        "      return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "\n",
        "  push_to_hub=False,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# specify model and model_init\n",
        "\n",
        "# Regression also works with AutoModelForSequenceClassification but with num_labels =1 . Then the compute_metrics will be different\n",
        "if do_hyperparameter_search:\n",
        "    model = None\n",
        "    def create_model_init():\n",
        "        return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "\n",
        "    model_init = create_model_init\n",
        "\n",
        "\n",
        "else:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "    model_init=None\n",
        "\n",
        "\n",
        "def define_trainer(encoded_train_set, encoded_val_set, model=None, model_init=None, compute_metrics = None, imbalanced = imbalanced):\n",
        "    if not imbalanced:\n",
        "        trainer = Trainer(\n",
        "            model = model,\n",
        "            model_init = model_init,\n",
        "            args = args,\n",
        "            train_dataset=encoded_train_set,\n",
        "            eval_dataset=encoded_val_set,\n",
        "            tokenizer=tokenizer,\n",
        "          compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # here we use a CustomTrainer for class weights\n",
        "        trainer = CustomTrainer(\n",
        "            model = model,\n",
        "            model_init = model_init,\n",
        "            args = args,\n",
        "            train_dataset=encoded_train_set,\n",
        "            eval_dataset=encoded_val_set,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "    return trainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define two more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the `Trainer` to load the best model it saved (according to `metric_name`) at the end of training.\n",
        "\n",
        "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/bert-finetuned-mrpc\"` or `\"huggingface/bert-finetuned-mrpc\"`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "The last thing to define for our `Trainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibWGmvxbIrJg"
      },
      "source": [
        "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it once last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a pad method that will do all of this right for us, and the `Trainer` will use it. You can customize this part by defining and passing your own `data_collator` which will receive the samples like the dictionaries seen above and will need to return a dictionary of tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlI6tq8l0d7z"
      },
      "source": [
        "# Train and save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEnt09bX0d7z",
        "outputId": "1e833958-45db-4dda-bae9-e9f72b0389dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2024-02-01 21:56:34,372] A new study created in memory with name: no-name-02894b56-ff85-4afc-8ad1-cb5a09dffc75\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "encoded_train_set = encoded_dataset[\"train\"]\n",
        "encoded_val_set = encoded_dataset[validation_key]\n",
        "\n",
        "if do_hyperparameter_search:\n",
        "    if hyperparameter_shards:\n",
        "        encoded_train_dataset_shards = encoded_train_set.shard(index=1, num_shards=hyperparameter_shards)\n",
        "        trainer = define_trainer(encoded_train_dataset_shards, encoded_val_set, model=model, model_init=model_init , compute_metrics = compute_metrics,imbalanced=imbalanced)\n",
        "    else:\n",
        "        trainer = define_trainer(encoded_train_set, encoded_val_set, model=model, model_init=model_init , compute_metrics = compute_metrics,imbalanced=imbalanced)\n",
        "    best_run = trainer.hyperparameter_search(n_trials=hyperparameter_search_n_trials,\n",
        "                                             direction=hyperparameter_search_direction,\n",
        "                                             backend='optuna',\n",
        "                                             hp_space = optuna_hp_space, # or will use default hp_space for whatever the backend is\n",
        "                                             )\n",
        "    print('==== best run:', best_run)\n",
        "    print()\n",
        "\n",
        "    # re define the trainer with the entire dataset\n",
        "    trainer = define_trainer(encoded_train_set, encoded_val_set, model=model, model_init=model_init , compute_metrics = compute_metrics, imbalanced=imbalanced)\n",
        "    for n, v in best_run.hyperparameters.items():\n",
        "        setattr(trainer.args, n, v)\n",
        "\n",
        "# train on entire training set for the last time\n",
        "trainer.train()\n",
        "\n",
        "ts_i = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
        "\n",
        "if toy:\n",
        "  output_dir_i = output_dir + f'results_{ts_i}_toy/'\n",
        "else:\n",
        "  output_dir_i = output_dir + f'results_{ts_i}/'\n",
        "os.makedirs(output_dir_i, exist_ok=True)\n",
        "\n",
        "trainer.save_model(output_dir_i)\n",
        "# trainer.push_to_hub() # need to have added token above in TrainingArguments()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load\n",
        "# from transformers import AutoModel, AutoConfig\n",
        "# config = AutoModelForSequenceClassification.from_pretrained(output_dir_i)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(output_dir_i, config=config)"
      ],
      "metadata": {
        "id": "_78MUpiQHuo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "# model_init=None\n",
        "\n",
        "\n",
        "trainer = define_trainer(encoded_train_set, encoded_val_set, model=model, model_init=model_init, compute_metrics = compute_metrics, imbalanced = imbalanced)"
      ],
      "metadata": {
        "id": "_ZxLA15SJ6vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_params = f\"weight_decay: {trainer.args.weight_decay}; num_train_epochs: {trainer.args.num_train_epochs}; learning_rate: {trainer.args.learning_rate}\"\n",
        "# best_params = \"weight_decay: 6.8120818721882775e-06; num_train_epochs: 4; learning_rate: 1.094605203258915e-06\"\n",
        "# best_params\n"
      ],
      "metadata": {
        "id": "8tXaD9d5GVdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_params = str(list(best_run.hyperparameters.items()))\n",
        "best_params"
      ],
      "metadata": {
        "id": "qVmVFVpAE_Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_9N-YlXJYli"
      },
      "source": [
        "The best run was:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXL9GD5z0d7z"
      },
      "source": [
        "# Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HePLtETwCP8w"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Regression\n",
        "if regression:\n",
        "  feature_vector = None\n",
        "  gridsearch = True\n",
        "\n",
        "  y_pred = trainer.predict(test_dataset=encoded_dataset[\"test\"]).predictions\n",
        "  y_pred = y_pred.reshape(1,-1)[0]\n",
        "\n",
        "\n",
        "  y_pred_df = pd.DataFrame(y_pred)\n",
        "  y_pred_df.to_csv(output_dir_i+f'y_pred_{model_name}_gridsearch-{gridsearch}_{ts_i}.csv', index=False)\n",
        "  path = output_dir_i + f'scatter_{model_name}_gridsearch-{gridsearch}_{ts_i}'\n",
        "\n",
        "  # Performance\n",
        "  results_i =regression_report(y_test,y_pred,y_train=y_train,\n",
        "                              metrics_to_report = metrics_to_report,\n",
        "                                gridsearch=gridsearch,\n",
        "                              best_params=best_params,feature_vector=feature_vector,model_name=model_name, plot = True, save_fig_path = path, round_to = 2)\n",
        "  results_i.to_csv(output_dir_i + f'results_{model_name}_gridsearch-{gridsearch}_{ts_i}.csv')\n",
        "  display(results_i)\n",
        "  results.append(results_i)\n",
        "  results_df = pd.concat(results)\n",
        "  results_df = results_df.reset_index(drop=True)\n",
        "  results_df.to_csv(output_dir_i + f'results_{ts_i}.csv', index=False)\n",
        "\n",
        "else:\n",
        "  # Classification\n",
        "  y_pred_proba = trainer.predict(test_dataset=encoded_dataset[\"test\"]).predictions\n",
        "\n",
        "  y_pred_proba = np.array(y_pred_proba)\n",
        "  y_pred_proba_1 = y_pred_proba[:,1]\n",
        "  y_pred = [np.argmax(n) for n in y_pred_proba]\n",
        "\n",
        "  test['y_pred_proba_0'] = y_pred_proba[:,0]\n",
        "  test['y_pred_proba_1'] = y_pred_proba[:,1]\n",
        "  test['y_pred'] = y_pred\n",
        "\n",
        "\n",
        "  test.to_csv(output_dir_i+'test_predictions.csv', index=False)\n",
        "\n",
        "  # y_pred = [int(np.argmax(n)) for n in y_pred_proba]\n",
        "  print(metrics.classification_report(y_pred=y_pred, y_true=y_test))\n",
        "  results = {}\n",
        "\n",
        "  clf_report_sklearn = metrics.classification_report(y_test,y_pred, output_dict=False) #evaluate #different than the output of cross_validate() above.\n",
        "  cm_df_meaning, cm_df, cm_df_norm = metrics_report_cm(y_test, y_pred, output_dir_results, model_name, ts, save=True)\n",
        "  clf_report = metrics_report_classification_report(y_test,y_pred,y_pred_proba_1, output_dir_results, model_name, ts)\n",
        "  # scores = cross_validate(pipe, X, y, scoring=['f1','precision', 'recall'], cv=cv, return_train_score=False) #train and evaluate\n",
        "  results[model_name] = {\n",
        "      'clf_report': clf_report,\n",
        "      'cm_df_meaning': cm_df_meaning,\n",
        "      'cm_df': cm_df,\n",
        "      'cm_df_norm': cm_df_norm,\n",
        "      'clf_report_sklearn': clf_report_sklearn\n",
        "\n",
        "  }\n",
        "  for k, v in results.get(model_name).items():\n",
        "    print(k)\n",
        "    if k != 'clf_report_sklearn':\n",
        "      display(v)\n",
        "    else:\n",
        "      print(v)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUw_6WnFIypk"
      },
      "outputs": [],
      "source": [
        "# trainer.evaluate(eval_dataset=encoded_dataset[\"test\"])\n",
        "# trainer.evaluate() # this is done on the validation set specificed in the Trainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQprbY6aBd99"
      },
      "outputs": [],
      "source": [
        "# load model:\n",
        "#  - https://discuss.huggingface.co/t/how-to-save-my-model-to-use-it-later/20568/6\n",
        "# - model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "# - then just load the trainer, but instead of training you just predict (here I'm training to fine tune, but if we load the fine tuned model , there's no need)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e2f08397badc4b27b1cffad695384468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6578001f8c444cb4b6b5d367ab2a0c44",
              "IPY_MODEL_d87614ecc68046feab06b15ead9f0ca9",
              "IPY_MODEL_25072428bfb8439f8198adc7a0fd7abd"
            ],
            "layout": "IPY_MODEL_805f44bd4f88406a96a3faed2ed0b891"
          }
        },
        "6578001f8c444cb4b6b5d367ab2a0c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cc44e5f1c6544f68d564317c42a74e2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8ce0a8aa32c84c54950cb9e996b2d4c1",
            "value": "Map: 100%"
          }
        },
        "d87614ecc68046feab06b15ead9f0ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a010f36dd5f74aea8faa28dc07d68f86",
            "max": 3280,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1606121df3d4b2ea4054b28fc88d1f8",
            "value": 3280
          }
        },
        "25072428bfb8439f8198adc7a0fd7abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570c8bd4369240c2ab125c467b0580dc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_44b3b51af6d84c63af8dbadf54a2f97f",
            "value": " 3280/3280 [00:08&lt;00:00, 401.53 examples/s]"
          }
        },
        "805f44bd4f88406a96a3faed2ed0b891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc44e5f1c6544f68d564317c42a74e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ce0a8aa32c84c54950cb9e996b2d4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a010f36dd5f74aea8faa28dc07d68f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1606121df3d4b2ea4054b28fc88d1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "570c8bd4369240c2ab125c467b0580dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44b3b51af6d84c63af8dbadf54a2f97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83029eaf472d46818def81c2e8264df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_117cdce9e8f54ab9b0b267ca2dd59da5",
              "IPY_MODEL_9f611d0811e64f83b305d2702ed87d6b",
              "IPY_MODEL_f60dbe5330044d65bdc145cf02074767"
            ],
            "layout": "IPY_MODEL_693a66ac796b463380fdf9367dadda50"
          }
        },
        "117cdce9e8f54ab9b0b267ca2dd59da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af986b9f32724dad993c4c94280d1591",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_993e6da535c64bf5b360e2e91c486ada",
            "value": "Map: 100%"
          }
        },
        "9f611d0811e64f83b305d2702ed87d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a051496948be45b1ae5ff23f18a7343d",
            "max": 823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a2407b1a2294b0aa3aa315519349607",
            "value": 823
          }
        },
        "f60dbe5330044d65bdc145cf02074767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392d84ccfce94e2d8092ef77c6c9b890",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d76ad8124dac408f80391205fd42cdda",
            "value": " 823/823 [00:01&lt;00:00, 528.01 examples/s]"
          }
        },
        "693a66ac796b463380fdf9367dadda50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af986b9f32724dad993c4c94280d1591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "993e6da535c64bf5b360e2e91c486ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a051496948be45b1ae5ff23f18a7343d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a2407b1a2294b0aa3aa315519349607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "392d84ccfce94e2d8092ef77c6c9b890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76ad8124dac408f80391205fd42cdda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fb39908b6c24f05ba94ffdb05b14576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daf74c75e9ca476ba612ca51a1bed8a4",
              "IPY_MODEL_00eca85de75143e5a9629938a1ad8443",
              "IPY_MODEL_141893389be84c42aef3d5e8bfdf4dd8"
            ],
            "layout": "IPY_MODEL_022d996cb35e462da753a5df523bb7a3"
          }
        },
        "daf74c75e9ca476ba612ca51a1bed8a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46935bbc83d14cf78b29a34c117fb453",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cf1620ae14bb465cb74333354233881d",
            "value": "Map: 100%"
          }
        },
        "00eca85de75143e5a9629938a1ad8443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f00b599a6ac3420cbc6a496e7172d45d",
            "max": 1071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76c332f0f3314aa2a91f57ff7b629378",
            "value": 1071
          }
        },
        "141893389be84c42aef3d5e8bfdf4dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc52b35d522f4a73b7bd8ecea2296765",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9a40a880c68e46c2836c548d41df13ea",
            "value": " 1071/1071 [00:01&lt;00:00, 897.69 examples/s]"
          }
        },
        "022d996cb35e462da753a5df523bb7a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46935bbc83d14cf78b29a34c117fb453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf1620ae14bb465cb74333354233881d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f00b599a6ac3420cbc6a496e7172d45d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76c332f0f3314aa2a91f57ff7b629378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc52b35d522f4a73b7bd8ecea2296765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a40a880c68e46c2836c548d41df13ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}