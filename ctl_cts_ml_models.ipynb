{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict severity using construct-text similarity on suicide risk lexicon\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import datetime\n",
    "from skopt import BayesSearchCV # had to replace np.int for in in transformers.py\n",
    "from sklearn import metrics\n",
    "\n",
    "# TODO: !pip install construct-tracker\n",
    "sys.path.append('./../construct-tracker/src/')\n",
    "sys.path.append('./../construct-tracker/src/construct_tracker/')\n",
    "\n",
    "from construct_tracker.machine_learning import feature_importance\n",
    "from construct_tracker.machine_learning.metrics_report import cm, classification_report, regression_report\n",
    "from construct_tracker.machine_learning.pipelines import get_params, get_pipelines, get_combinations\n",
    "import srl_constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'local' \n",
    "\n",
    "if location == 'colab':\n",
    "  from google.colab import drive\n",
    "  project_name = 'concept_tracker'\n",
    "  drive.mount('/content/drive')\n",
    "  input_dir = f'/content/drive/MyDrive/datum/{project_name}/data/ctl/'\n",
    "  output_dir = f'/content/drive/MyDrive/datum/{project_name}/data/output/lexicon_paper/'\n",
    "elif location == 'openmind':\n",
    "  input_dir = '/nese/mit/group/sig/projects/dlow/ctl/'\n",
    "  output_dir = '/home/dlow/datum/lexicon/data/output/mpnet/'\n",
    "elif location =='local':\n",
    "  input_dir = './data/input/ctl/'\n",
    "  output_features_dir = './data/input/ctl/'\n",
    "  output_dir = './data/output/ml_performance/cts/'\n",
    "  \n",
    "\n",
    "os.makedirs(output_features_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srl_constructs\n",
    "\n",
    "constructs = srl_constructs.constructs_in_order\n",
    "constructs_max = [n+'_max' for n in constructs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function with a threshold argument\n",
    "def transform_value(x, threshold):\n",
    "    integer_part = np.floor(x)\n",
    "    decimal_part = x - integer_part\n",
    "    if decimal_part < threshold:\n",
    "        return integer_part\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_features_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # better embeddingsa\n",
    "\n",
    "\n",
    "# X_train_df_sim = pd.read_csv(output_features_dir+\"cts-scores_count-False_thresh-0_train10_train_30perc_text_y_balanced_regression_24-08-16T19-48-36/cts_scores.csv\")\n",
    "# X_test_df_sim = pd.read_csv(output_features_dir+\"cts-scores_count-False_thresh-0_train10_test_15perc_text_y_regression_24-08-16T21-51-23/cts_scores.csv\")\n",
    "\n",
    "\n",
    "# counts_dir = 'suicide_risk_lexicon_v1.0_counts_and_matches_24-08-15T19-04-55/'\n",
    "\n",
    "# # count only prototypes\n",
    "# X_train_df_counts = pd.read_csv(output_features_dir+counts_dir+'suicide_risk_lexicon_v1.0_counts_train10_train_30perc_text_y_balanced_regression.csv')\n",
    "# X_test_df_counts = pd.read_csv(output_features_dir+counts_dir+'suicide_risk_lexicon_v1.0_counts_train10_test_15perc_text_y_regression.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all-mini-lm-v6\n",
    "counts_dir  = 'suicide_risk_lexicon_v1.0_counts_and_matches_24-08-15T19-04-55/'\n",
    "\n",
    "X_train_df_counts = pd.read_csv(output_features_dir+counts_dir+'suicide_risk_lexicon_v1.0_counts_train10_train_30perc_text_y_balanced_regression.csv')\n",
    "X_test_df_counts = pd.read_csv(output_features_dir+counts_dir+'suicide_risk_lexicon_v1.0_counts_train10_test_15perc_text_y_regression.csv')\n",
    "\n",
    "# # count only prototypes\n",
    "X_train_df_sum = pd.read_csv(output_features_dir+'cts-scores_count-sum_thresh-03_24-08-14T16-48-28/train10_train_30perc_text_y_balanced_regression_cts-scores.csv')\n",
    "X_test_df_sum = pd.read_csv(output_features_dir+'cts-scores_count-sum_thresh-03_train10_test_15perc_text_y_regression_24-08-14T22-50-58/cts_scores.csv')\n",
    "\n",
    "X_train_df_sim = X_train_df_sum.copy()\n",
    "X_train_df_sim[constructs_max] = X_train_df_sum[constructs_max].mod(1)\n",
    "X_test_df_sim = X_test_df_sum.copy()\n",
    "X_test_df_sim[constructs_max] = X_test_df_sum[constructs_max].mod(1)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(X_test_df_sum[constructs_max].values.ravel(), bins = 2000)\n",
    "# plt.xlim((-1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if they match in rows and values to the concat. \n",
    "\n",
    "# assert X_train_df_sim.shape[0] == X_train_df_counts.shape[0]\n",
    "# assert X_test_df_sim.shape[0] == X_test_df_counts.shape[0]\n",
    "\n",
    "# assert X_train_df_sim['conversation_id'].tolist() == X_train_df_counts['conversation_id'].tolist()\n",
    "# assert X_test_df_sim['conversation_id'].tolist() == X_test_df_counts['conversation_id'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_sum = X_train_df_sim.copy()\n",
    "X_train_df_sum[constructs_max] = X_train_df_sum[constructs_max].values+X_train_df_counts[constructs].values\n",
    "\n",
    "X_test_df_sum = X_test_df_sim.copy()\n",
    "X_test_df_sum[constructs_max] = X_test_df_sum[constructs_max].values+X_test_df_counts[constructs].values\n",
    "\n",
    "X_test_df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_test_df_sum[constructs_max].values.ravel(), bins = 2000)\n",
    "plt.xlim((-1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create different datasets:\n",
    "\n",
    "- thresholds: 0, 0.3, 0.45, 0.6, 0.75\n",
    "- counts: sim, counts, round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = {\n",
    "# \t'counts': [X_train_df_counts, X_test_df_counts, constructs],\n",
    "# \t'sim' : [X_train_df_sim, X_test_df_sim, constructs_max],\n",
    "# \t'sum': [X_train_df_sum, X_test_df_sum, constructs_max],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'sim'\n",
    "# threshold_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Apply the function to each element in the DataFrame with a threshold argument\n",
    "\n",
    "\n",
    "# if dataset == 'sum_all_tokens':\n",
    "\n",
    "# \t# remove counts, just similarites\n",
    "# \tX_train_df[srl_constructs.constructs_in_order] = X_train_df[srl_constructs.constructs_in_order].mod(1)\n",
    "\n",
    "# \t# add new counts\n",
    "# \tX_train_df[srl_constructs.constructs_in_order] =X_train_df[srl_constructs.constructs_in_order].values+ X_train_df_counts[srl_constructs.constructs_in_order].values\n",
    "\n",
    "\n",
    "# \t# remove counts, just similarites\n",
    "# \tX_test_df[srl_constructs.constructs_in_order] = X_test_df[srl_constructs.constructs_in_order].mod(1)\n",
    "\n",
    "# \t# add new counts\n",
    "# \tX_test_df[srl_constructs.constructs_in_order] = X_test_df[srl_constructs.constructs_in_order].values+ X_test_df_counts[srl_constructs.constructs_in_order].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lbl2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from construct_tracker import lexicon\n",
    "# srl = lexicon.load_lexicon(name = 'srl_v1-0')\n",
    "srl_prototypes = lexicon.load_lexicon(name = 'srl_prototypes_v1-0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "counts_dir  = 'suicide_risk_lexicon_v1.0_counts_and_matches_24-08-15T19-04-55/'\n",
    "\n",
    "X_train_df_counts = pd.read_csv(output_features_dir+counts_dir+'suicide_risk_lexicon_v1.0_counts_train10_train_30perc_text_y_balanced_regression.csv')\n",
    "X_test_df_counts = pd.read_csv(output_features_dir+counts_dir+'suicide_risk_lexicon_v1.0_counts_train10_test_15perc_text_y_regression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbl2vec import Lbl2TransformerVec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "toy = False\n",
    "\n",
    "lexicon_dict = srl_prototypes.to_dict() \n",
    "\n",
    "if toy:\n",
    "\t\n",
    "\tlexicon_dict = dict(random.sample(list(lexicon_dict.items()), 2))\n",
    "constructs = list(lexicon_dict.keys())\n",
    "\n",
    "\t\n",
    "# select sentence-tranformers model\n",
    "transformer_model = SentenceTransformer('all-MiniLM-L6-v2') #\"all-mpnet-base-v2\"\n",
    "\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for split, name in [\n",
    "\t(X_train_df_counts, 'train'),\n",
    "\t(X_test_df_counts, 'test'),\n",
    "\n",
    "]:\n",
    "\n",
    "\t# init model\n",
    "\tif toy: \n",
    "\t\tdocuments = split['document'].values[:10]\n",
    "\telse:\n",
    "\t\tdocuments = split['document'].values\n",
    "\n",
    "\tmodel = Lbl2TransformerVec(transformer_model=transformer_model,\n",
    "\t\t\t\t\t\t\tkeywords_list=list(lexicon_dict.values()), # iterable list of lists with descriptive keywords of type str\n",
    "\t\t\t\t\t\t\tdocuments=documents, #iterable list of strings\n",
    "\t\t\t\t\t\t\tlabel_names = list(lexicon_dict.keys())\n",
    "\t\t\t\t\t\t\t)\n",
    "\n",
    "\t# train model\n",
    "\tmodel.fit()\n",
    "\n",
    "\tsimilarity_df = model.predict_model_docs() # Predict label similarities for documents used for training\n",
    "\t# similarity_df = model.predict_model_docs(doc_idxs = [0,1]) # Predict label similarities for documents used for training\n",
    "\n",
    "\t# split_sum = split.copy()\n",
    "\t\n",
    "\t# datasets[split] = [split, similarity_df, split_sum]\n",
    "\tsplit_sim = split.copy()\n",
    "\tsplit_sim[constructs] = similarity_df[constructs]\n",
    "\n",
    "\tsplit_sum = split.copy()\n",
    "\tsplit_sum[constructs] += similarity_df[constructs]\n",
    "\t\n",
    "\tdatasets[name] = [split, similarity_df, split_sim, split_sum]\n",
    "\t# model.save(output_dir + name)\n",
    "\t# model = Lbl2Vec.load('model_name')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train_df, X_test_df, constructs_cols = datasets.get(dataset)\n",
    "# X_train_df, X_test_df, constructs_cols = X_train_df_sim, X_test_df_sim, constructs_max\n",
    "# X_train_df, X_test_df, constructs_cols = datasets.get(dataset), constructs\n",
    "split_train, similarity_df_train, split_sim_train, split_sum_train = datasets.get('train')\n",
    "split_test, similarity_df_test, split_sim_test, split_sum_test = datasets.get('test')\n",
    "constructs_cols = constructs\n",
    "X_train_df = split_sim_train.copy()\n",
    "X_test_df = split_sim_test.copy()\n",
    "threshold_value = 0\n",
    "\n",
    "# Clean up column names\n",
    "\n",
    "# X_train_df.columns = [x.replace('_max', '') for x in X_train_df.columns]\n",
    "# X_test_df.columns = [x.replace('_max', '') for x in X_test_df.columns]\n",
    "\n",
    "# Keep as DF with column names\n",
    "X_train = X_train_df[constructs_cols]\n",
    "X_test = X_test_df[constructs_cols]\n",
    "\n",
    "y_train = X_train_df['y'].values\n",
    "y_test = X_test_df['y'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if threshold_value:\n",
    "\tX_train = X_train.applymap(lambda x: transform_value(x, threshold_value))\n",
    "\tX_test = X_test.applymap(lambda x: transform_value(x, threshold_value))\n",
    "\n",
    "\n",
    "# Just the decimel\n",
    "# X_train.mod(1)\n",
    "\n",
    "# just the integer\n",
    "\n",
    "# X_train = X_train.applymap(np.floor).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = False\n",
    "\n",
    "# Config\n",
    "balance = True # balance training set by downsampling\n",
    "task = 'regression'\n",
    "target = 'immiment_risk' # [1,2,3]\n",
    "normalize_lexicon = True\n",
    "\n",
    "if task == 'classification':\n",
    "\tdv = 'suicide_ladder_classification'\n",
    "\tif target == 'suicidal_desire':\n",
    "\t\tbalance_values = ['nonsuicidal','suicidal_desire']\n",
    "\telif target == 'imminent_risk':\n",
    "\t\tbalance_values = ['suicidal_desire','imminent_risk']\n",
    "\tsmallest_value = 'imminent_risk'\n",
    "\tn = 1893\n",
    "\n",
    "elif task == 'regression':\n",
    "\n",
    "\t# config\n",
    "\tdv = 'suicide_ladder_a'\n",
    "\tbalance_values = [1,2,3]\n",
    "\tsmallest_value = 3\n",
    "\n",
    "feature_vectors = [\n",
    "\t# 'all-MiniLM-L6-v2', \n",
    "\t# 'srl_validated',\n",
    "\t# 'srl_unvalidated',\n",
    "\t# 'SRL GPT-4 Turbo', \n",
    "\t# 'liwc22', \n",
    "\t# 'liwc22_semantic'\n",
    "\t'cts_thresh-03_count-sum',\n",
    "\t] # srl_unvalidated_text_descriptives','text_descriptives' ]\n",
    "\n",
    "sample_sizes = ['all'] # # TODO: sample_sizes = ['all', 150] \n",
    "\n",
    "if task == 'classification':\n",
    "\tscoring = 'f1'\n",
    "\tmetrics_to_report = 'all'\n",
    "\tmodel_names = ['LGBMRegressor'] # model_names = ['LGBMRegressor', 'LogisticRegression'] \n",
    "\t\n",
    "elif task == 'regression':\n",
    "\tscoring = 'neg_mean_squared_error'\n",
    "\t# metrics_to_report = ['Model','n', 'RMSE','RMSE per value','MAE','MAE per value',  'rho', 'gridsearch', 'Best parameters']\n",
    "\tmodel_names = ['LGBMRegressor']  # TODO: 'Ridge']\n",
    "\tmetrics_to_report = 'all'\n",
    "\n",
    "gridsearch = True#, 'minority'\n",
    "balance = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for similarity_threshold in [None, 0.3, 0.45, 0.6] # Missing None\n",
    "# for lexicon in [srl, srl_prototypes, just examples] \n",
    "# for count_if_exact_match in [False,  'replace', 'sum', 'sum1']\n",
    "# this can be done with the same features dataset.\n",
    "# for count_if_exact_match in [just decimal,  just int, 'sum', if>0=1, else nothing]\n",
    "\n",
    "# CTS similarities \n",
    "# CTS replace with count\n",
    "# CTS sum (similarites + count): if this doesnt work, others might not work\n",
    "# start with prototype\n",
    "# CTS sum 1 (similarites + 1 if exact match): need to implement: > 1 ==1 (easy), might give best result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# TODO: see where to save feature_vector (tfidf, liwc22) and where to save model_name (Ridge, LightGBM)\n",
    "\n",
    "\n",
    "\n",
    "ts_i = datetime.datetime.utcnow().strftime('%y-%m-%dT%H-%M-%S')\n",
    "\n",
    "if toy:\n",
    "\tsample_sizes = [150]\n",
    "\tfeature_vectors = feature_vectors[:2]\n",
    "\n",
    "for n in sample_sizes:\n",
    "\tresults = []\n",
    "\t# for gridsearch in [True]:\n",
    "\n",
    "\t# for feature_vector in ['srl_unvalidated', 'all-MiniLM-L6-v2']:#['srl_unvalidated']:#, 'srl_unvalidated']:\n",
    "\tfor feature_vector in feature_vectors:#['srl_unvalidated']:#, 'srl_unvalidated']:\n",
    "\t\toutput_dir = output_dir+feature_vector+'/'\n",
    "\t\tos.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\t\tif toy:\n",
    "\t\t\toutput_dir_i = output_dir + f'results_{ts_i}_toy/'\n",
    "\t\telse:\n",
    "\t\t\toutput_dir_i = output_dir + f'results_{ts_i}_{n}_{task}_{balance_values[-1]}/'\n",
    "\t\t\t\n",
    "\t\tos.makedirs(output_dir_i, exist_ok=True)\n",
    "\t\t\n",
    "\t\n",
    "\t\tif toy:\n",
    "\t\t\t\n",
    "\t\t\tX_train['y'] = y_train\n",
    "\t\t\tX_train = X_train.sample(n = 100)\n",
    "\t\t\ty_train = X_train['y'].values\n",
    "\t\t\tX_train = X_train.drop('y', axis=1)\n",
    "\t\n",
    "\t\telif n!='all':\n",
    "\t\t\tX_train['y'] = y_train\n",
    "\t\t\tX_train = X_train.sample(n = n, random_state=123)\n",
    "\t\t\ty_train = X_train['y'].values\n",
    "\t\t\tX_train = X_train.drop('y', axis=1)\n",
    "\t\n",
    "\n",
    "\t\tif task == 'classification':\n",
    "\t\t\tencoder = LabelEncoder()\n",
    "\n",
    "\t\t\t# Fit and transform the labels to integers\n",
    "\t\t\ty_train = encoder.fit_transform(y_train)\n",
    "\t\t\ty_test = encoder.transform(y_test)\n",
    "\n",
    "\t\t\n",
    "\t\tfor model_name in model_names: \n",
    "\t\n",
    "\t\t\tpipeline = get_pipelines(feature_vector, model_name = model_name)\n",
    "\t\t\tprint(pipeline)\n",
    "\t\t\n",
    "\t\t\t# I'd need y_val \n",
    "\n",
    "\t\t\t# if gridsearch == 'minority':\n",
    "\t\t\t# \t# Obtain all hyperparameter combinations\n",
    "\t\t\t# \tparameters = get_params(feature_vector,model_name=model_name, toy=toy)\n",
    "\t\t\t# \tparameter_set_combinations = get_combinations(parameters)\n",
    "\t\t\t# \tscores = {}\n",
    "\t\t\t# \tfor i, set in enumerate(parameter_set_combinations):\n",
    "\t\t\t# \t\tpipeline.set_params(**set)\n",
    "\t\t\t# \t\tpipeline.fit(X_train,y_train)\n",
    "\t\t\t# \t\ty_pred = pipeline.predict(X_val) # validation set \n",
    "\t\t\t# \t\trmse_per_value = []\n",
    "\t\t\t# \t\trmse = metrics.mean_squared_error(y_val, y_pred, squared=False ) # validation set \n",
    "\t\t\t# \t\tfor value in np.unique(y_val):\n",
    "\t\t\t# \t\t\ty_pred_test_i = [[pred,test] for pred,test in zip(y_pred,y_val) if test == value] # validation set \n",
    "\t\t\t# \t\t\ty_pred_i = [n[0] for n in y_pred_test_i]\n",
    "\t\t\t# \t\t\ty_test_i = [n[1] for n in y_pred_test_i]\n",
    "\t\t\t# \t\t\trmse_i = metrics.mean_squared_error(y_test_i, y_pred_i, squared=False )\n",
    "\t\t\t# \t\t\trmse_per_value.append(rmse_i )\n",
    "\t\t\t# \t\tscores[i] = [rmse]+rmse_per_value+[str(set)]\n",
    "\t\t\t# \tscores = pd.DataFrame(scores).T\n",
    "\t\t\t# \tscores.columns = ['RMSE', 'RMSE_2', 'RMSE_3', 'RMSE_4', 'Parameters']\n",
    "\t\t\t# \tscores = scores.sort_values('RMSE_4')\n",
    "\t\t\t# \tbest_params = eval(scores['Parameters'].values[0])\n",
    "\t\t\t# \tpipeline.set_params(**best_params)\n",
    "\t\t\t# \tpipeline.fit(X_train,y_train)\n",
    "\t\t\t# \ty_pred = pipeline.predict(X_test)\n",
    "\t\t\t\t\n",
    "\t\t\tif gridsearch == True:\n",
    "\t\t\t\tparameters = get_params(feature_vector,model_name=model_name, toy=toy)\n",
    "\t\n",
    "\t\t\t\tpipeline = BayesSearchCV(pipeline, parameters, cv=5, scoring=scoring, return_train_score=False,\n",
    "\t\t\t\tn_iter=32, random_state=123)    \n",
    "\t\t\t\tif feature_vector != 'tfidf':\n",
    "\t\t\t\t\tif 'y' in X_train.columns:\n",
    "\t\t\t\t\t\twarnings.warn('y var is in X_train, removing')\n",
    "\t\t\t\t\t\tX_train = X_train.drop('y', axis=1)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tpipeline.fit(X_train,y_train)\n",
    "\t\t\t\tbest_params = pipeline.best_params_\n",
    "\t\t\t\tbest_model = pipeline.best_estimator_\n",
    "\t\t\t\tif feature_vector != 'tfidf':\n",
    "\t\t\t\t\tif 'y' in X_test.columns:\n",
    "\t\t\t\t\t\twarnings.warn('y var is in X_test, removing')\n",
    "\t\t\t\t\t\tX_test = X_test.drop('y', axis=1)\n",
    "\t\t\t\ty_pred = best_model.predict(X_test)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpipeline.fit(X_train,y_train)\n",
    "\t\t\t\tbest_params = 'No hyperparameter tuning'\n",
    "\t\t\t\ty_pred = pipeline.predict(X_test)\n",
    "\t\t\t\n",
    "\t\t\t# Predictions\n",
    "\t\t\t# output_filename = f'{feature_vector}_{model_name}_{classes_i[1]}_{n}_clauses-{amount_of_clauses}'\n",
    "\t\t\t# custom_cr, sklearn_cr, cm_df_meaning, cm_df, cm_df_norm, y_pred_df = metrics_report.save_classification_performance(y_test_i, y_pred, y_proba_1, output_dir_i, output_filename=output_filename,feature_vector=feature_vector, model_name=model_name,best_params = best_params, classes = classes_i,amount_of_clauses=amount_of_clauses, save_output=True)\n",
    "\t\t\t\n",
    "\t\t\ty_pred_df = pd.DataFrame(y_pred)\n",
    "\t\t\ty_pred_df.to_csv(output_dir_i+f'y_pred_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}.csv', index=False)\n",
    "\t\t\tpath = output_dir_i + f'scatter_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}'\n",
    "\t\t\n",
    "\t\t\t# Performance\n",
    "\t\t\tif task == 'classification':\n",
    "\t\t\t\tcm_df_meaning, cm_df, cm_df_norm = cm(y_test,y_pred, output_dir_i, model_name, ts_i, classes = balance_values, save=True)\n",
    "\t\t\t\ty_proba = pipeline.predict_proba(X_test)       # Get predicted probabilities\n",
    "\t\t\t\ty_proba_1 = y_proba[:,1]\n",
    "\t\t\t\ty_pred = y_proba_1>=0.5*1                   # define your threshold\n",
    "\t\t\t\tresults_i = classification_report(y_test, y_pred, y_proba_1, output_dir_i,gridsearch=gridsearch,\n",
    "\t\t\t\t\t\t\t\t\t\tbest_params=best_params,feature_vector=feature_vector,model_name=model_name,round_to = 2, ts = ts_i)\n",
    "\t\t\telif task == 'regression':\n",
    "\n",
    "\t\t\t\tresults_i =regression_report(y_test,y_pred,y_train=y_train,\n",
    "\t\t\t\t\t\t\t\t\t\tmetrics_to_report = metrics_to_report,\n",
    "\t\t\t\t\t\t\t\t\t\t\tgridsearch=gridsearch,\n",
    "\t\t\t\t\t\t\t\t\t\tbest_params=best_params,feature_vector=feature_vector,model_name=model_name, plot = True, save_fig_path = path,n = n, round_to = 2)\n",
    "\t\t\tresults_i.to_csv(output_dir_i + f'results_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}.csv')\n",
    "\t\t\tdisplay(results_i)\n",
    "\t\t\tresults.append(results_i)\n",
    "\t\t\tresults_df = pd.concat(results)\n",
    "\t\t\tresults_df = results_df.reset_index(drop=True)\n",
    "\t\t\tresults_df.to_csv(output_dir_i + f'results_{n}_{ts_i}.csv', index=False)\n",
    "\t\t\n",
    "\t\t\t# Feature importance\n",
    "\t\t\tif feature_vector == 'tfidf':\n",
    "\t\t\t\tif model_name in ['XGBRegressor']:\n",
    "\t\t\t\t\twarnings.warn('Need to add code to parse XGBoost feature importance dict')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfeature_importances = feature_importance.tfidf_feature_importances(pipeline, top_k = 50, savefig_path = output_dir_i + f'feature_importance_{feature_vector}_{model_name}_{n}_{ts_i}')\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeature_names = X_train.columns\n",
    "\t\t\t\tfeature_importance_df = feature_importance.generate_feature_importance_df(pipeline, model_name,feature_names,  xgboost_method='weight', model_name_in_pipeline = 'model')\n",
    "\t\t\t\tif str(feature_importance_df) != 'None':       # I only implemented a few methods for a few models\n",
    "\t\t\t\t\tfeature_importance_df.to_csv(output_dir_i + f'feature_importance_{feature_vector}_{model_name}_gridsearch-{gridsearch}_{n}_{ts_i}.csv', index = False)        \n",
    "\t\t\t\t\t# display(feature_importance.iloc[:50])\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\t\t# NaN analysis\n",
    "\t\t\tif type(X_train) == pd.core.frame.DataFrame:\n",
    "\t\t\t\tdf = X_train.copy()\n",
    "\t\t\t\t# Find the column and index of NaN values\n",
    "\t\t\t\tnan_indices = df.index[df.isnull().any(axis=1)].tolist()\n",
    "\t\t\t\tnan_columns = df.columns[df.isnull().any()].tolist()\n",
    "\t\t\t\t# print(\"Indices of NaN values:\", nan_indices)\n",
    "\t\t\t\tprint(\"Columns with NaN values:\", nan_columns)\n",
    "\t\t\t\tprint(df.size)\n",
    "\t\t\t\tnans = df.isna().sum().sum()\n",
    "\t\t\t\tprint('% of nans:', np.round(nans/df.size,3))\n",
    "\t\t\t\n",
    "\t\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concept_tracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
